===============================
Parallel MCMC
===============================


.. image:: https://img.shields.io/travis/mchakra2/parallelmcmc.svg
        :target: https://travis-ci.org/mchakra2/parallelmcmc

.. image:: https://pyup.io/repos/github/mchakra2/parallelmcmc/shield.svg
	:target: https://pyup.io/repos/github/mchakra2/parallelmcmc/
	:alt: Updates

.. image:: https://coveralls.io/repos/github/mchakra2/parallelmcmc/badge.svg
        :target: https://coveralls.io/github/mchakra2/parallelmcmc




This is the parallelized version of the Markov Chain Monte Carlo (MCMC) project created previously.


* Free software: MIT license
* Documentation: https://mcmc.readthedocs.io.


Features
--------
All the functions are same but they have been rewritten to reduce usage of class variable to support parallization.
The parallel code has been added to the main function.

* input_arg: function to read in parameters and node tuples from the input file
* dist: function to calculate the distance between two grid points or tuples
* make_init_graph: function to make an initial connected graph from the given tuples
* graph_change: function to either add or remove an edge
* calculate_bridges: function to calculate the number of bridges in a graph
* calculate_q: function to calculate qij and qji
* theta_func: function to calculate theta, which is subsequently used for calculating the relative probability pi_j/pi_i
* MH: function that accepts or rejects the proposed change in graph according to the Metropolis Hastings algorithm
* max_shortest_path: function which return the longest among the shortest paths between vertex 0 and all other vertices
* graph_count: function to keep track of unique graphs encountered during the simulation using a dictionary
* mc_chain_generator: function to generate the markov chain for the given number of iterations
* quantiling: function to return the edge list of 1% most probable graphs  
* main: The main function

Implementation Details
~~~~~~~~~~~~~~~~~~~~~~~
The implementation approach remains the same with the addition of a parallelized section.
A new connected graph (G2) is generated by mutating the state of a randomly selected edge (from all possible edges for the given nodes) of the existing graph (G1). So, if the selected edge is present in G1, it is removed if removal of that edge does not disconnect the graph. If the randomly selected edge is not present in G1, the edge is added. In the code, random edge selection is done by randomly selecting two node tuples from tuple list M without replacement. qij,qji and the relative probability of pi_j/pi_i are calculated. G2 is either accepted or rejected according to the Metropolis Hastings algorithm. The edge list of unique graph encountered during the simulation is added to a dictionary. For each additional observation of an already observed graph, the value of the key in the dictionary is incremented by 1. Throughout the simulation a running sum of the following are maintained: degree of vertex 0, total number of edges in the G1 and the longest path from vertex 0 to any other vertex. Thoughout the implementation, the first node tuple in the nodelist M is considered as vertex 0. The weight of an edge between two vertices is the Euclidian distance between the two node tuples. The expected values of an attribute is calculated at the end of the simulation by dividing the running sum by the number of iterations. The top 1% of most probable graphs is given in the output file as edge lists. When the number of observed unique graphs is less than 100, only the most likely graph is provided as output. Default parameter values are coded in the **mcmc.py** file in **mcmc** sub-directory of the package. The user has to provide an input file named **input.txt** which contains the following information:


* parameter r (optional)
* parameter T (optional)
* process_num (optional): Number of processors to be used. Default number is the number of processors in your machine.  
* iterations: This is the number of steps in the monte carlo simulation
* location of the output file     
* nodes as tuples (e.g. 2,2 .Each node tuple should be written in a separate line.)

The input file must be saved in the **IOFiles** subdirectory. The **IOFiles** subdirecroty already contains an example input file. Please note that the keys identifying the parameters in the input file should not be altered. To run the script, clone the package by typing this in your command line:
  
git clone https://github.com/mchakra2/parallelmcmc


Adjust the **input.txt** file in **IOFiles** subdirectory to your liking. To run the mcmc script, make sure you are in the main **parallelmcmc** package directory (but out of the **mcmc** subdirectory!). Type the following:

python3 ./mcmc/__init__.py

You will find the output file in the location that you have mentioned in **input.txt** file. If you did not specify the name and location of the output file, you will find the **output.txt**  file under subdirectory **IOFiles** (because that is the default output file location!).   

**NOTE: Using Python2 seems to give a PicklingError while trying to execute the __init__.py file. However, the problem can be circumvented by using Python3** 

Comparison of Statistics
------------------------
The statistical values from both sequential  and parallel versions of the program agree to certain extent. The total number of iterations for both the cases was 10000. The results obtained by running the program with one processor (sequential) are as follows:

* Expectation value of the degree of vertex 0: 2.0368
* Expectation value of the total number of edges: 5.6459
* Expectaion of the maximum of the shortest paths from vertex 0 to other vertices: 5.31779

The results obtained by implementing parallelization using 8 processors are as follows:
  
* Expectation value of the degree of vertex 0: 2.2233
* Expectation value of the total number of edges: 5.6525
* Expectaion of the maximum of the shortest paths from vertex 0 to other vertices: 5.22066
  

  

Comments on Speed-Up
-------------------

With 1 processor the unit tests were completed in 37.735s. With 8 processors the unit tests were completed in 17.440s. The runtime of the program using 1 processor (T\ :sub:`1`) was 16.895s.
The run time of the parallelized version with more than 1 processors and their corresponding speedups are listed below:

* T\ :sub:`2`: 8.4682s, Speedup:2
* T\ :sub:`3`: 5.7166s, Speedup:2.96
* T\ :sub:`4`: 4.6189s, Speedup:3.66
* T\ :sub:`5`: 4.5384s, Speedup:3.72
* T\ :sub:`6`: 4.4768s, Speedup:3.77
* T\ :sub:`7`: 4.2916s, Speedup:3.94
* T\ :sub:`8`: 4.0723s, Speedup:4.15

    
Credits
---------

**Maghesree Chakraborty** - **mchakra2@ur.rochester.edu**
Special thanks to Dr. A White for being an excellent guide. 

This package was created with Cookiecutter_ and the `audreyr/cookiecutter-pypackage`_ project template.

.. _Cookiecutter: https://github.com/audreyr/cookiecutter
.. _`audreyr/cookiecutter-pypackage`: https://github.com/audreyr/cookiecutter-pypackage
